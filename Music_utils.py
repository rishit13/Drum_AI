import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchtext.data.metrics import bleu_score
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data.dataset import random_split
from torch.utils.data import Dataset, TensorDataset
from Seq_Model import EncoderLSTM,DecoderLSTM,Seq2Seq
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from music21 import converter,instrument,note,chord,stream
import os
import constants
from data_prep import int_tokens, data_batch_load, notes_to_index_fn
from Generate_drum_beats import preprocess_input, generate_beats
import numpy as np

def extract(parsed_notes):
    notes_tar = []
    for element in parsed_notes:
            if isinstance(element, note.Note):
                notes_tar.append([element.offset,str(element.pitch)])
            if isinstance(element, chord.Chord):
                notes_tar.append([element.offset,'.'.join(str(n) for n in element.normalOrder)])
    return notes_tar


def get_notes(filename):
    res_list_in = []
    notes_in = []
    notes_to_parse_in = None
    file = converter.parse(filename)
    s2 = instrument.partitionByInstrument(file)
    index_list = [0] * len(s2.parts)
    count_flag = 0
    for index, part in enumerate(s2.parts):
        index_list[index] = str(part)

    if (any('Electric Bass' in x for i, x in enumerate(index_list)) and count_flag < 1):
        index = [i for i, x in enumerate(index_list) if ('Electric Bass' in x)]
        notes_to_parse_in = s2.parts[index[0]].recurse()
        notes_in = extract(notes_to_parse_in)
        count_flag += 1

    if (any('Acoustic Bass' in x for i, x in enumerate(index_list)) and count_flag < 1):
        index = [i for i, x in enumerate(index_list) if ('Acoustic Bass' in x)]
        notes_to_parse_in = s2.parts[index[0]].recurse()
        notes_in = extract(notes_to_parse_in)
        count_flag += 1

    if (any('Bass' in x for i, x in enumerate(index_list)) and count_flag < 1):
        index = [i for i, x in enumerate(index_list) if ('Bass' in x)]
        notes_to_parse_in = s2.parts[index[0]].recurse()
        notes_in = extract(notes_to_parse_in)

    if (count_flag == 1):
        res_dict_in = dict((element[0], []) for element in notes_in)
        for value in notes_in:
            res_dict_in[value[0]].append(value[1])
        for key, value in res_dict_in.items():
            if float(key) > 30 and float(key) < 100:
                res_list_in.append(value)
        new_list_in = [item for sublist in res_list_in for item in sublist]
        return np.array(new_list_in)


def convert_to_midi(prediction_output,file):
    offset = 30
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:

        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                cn = int(current_note)
                new_note = note.Note(cn)
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset

            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offset += 1
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=file + "_drums" + '.mid')
    #new_path = new_file_path+ '.mid'
    #soundfont = '/Users/rishitdholakia/Library/Audio/Sounds/Banks/MuseScore_General.sf2'
    #fs = FluidSynth(sound_font=soundfont, sample_rate=22050)
    #fs.midi_to_audio(new_path, path + file + "drums" + '.wav')